{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:09:40.504375Z","iopub.execute_input":"2025-03-08T14:09:40.504666Z","iopub.status.idle":"2025-03-08T14:09:40.82132Z","shell.execute_reply.started":"2025-03-08T14:09:40.504644Z","shell.execute_reply":"2025-03-08T14:09:40.820393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.model_selection import train_test_split\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\n# GPUが利用可能か確認\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:09:40.822529Z","iopub.execute_input":"2025-03-08T14:09:40.822933Z","iopub.status.idle":"2025-03-08T14:10:05.177045Z","shell.execute_reply.started":"2025-03-08T14:09:40.822909Z","shell.execute_reply":"2025-03-08T14:10:05.175968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# データの読み込み\ntrain_data = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/train.csv')\ntest_data = pd.read_csv(r'/kaggle/input/llm-classification-finetuning/test.csv')\nlabels = np.zeros(len(train_data), dtype=int)\nlabels[train_data['winner_model_a'] == 1] = 0  # Aの勝ち\nlabels[train_data['winner_model_b'] == 1] = 1  # Bの勝ち\nlabels[train_data['winner_tie'] == 1] = 2    # 同点\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:10:05.17854Z","iopub.execute_input":"2025-03-08T14:10:05.179085Z","iopub.status.idle":"2025-03-08T14:10:08.512077Z","shell.execute_reply.started":"2025-03-08T14:10:05.17906Z","shell.execute_reply":"2025-03-08T14:10:08.511094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_better_worse_responses(train_data):\n    \"\"\"\n    トレーニングデータから好まれる回答と好まれない回答のペアを抽出する関数\n    \n    Args:\n        train_data (pd.DataFrame): 元のトレーニングデータフレーム\n        \n    Returns:\n        tuple: (prompts, better_responses, worse_responses)のタプル\n    \"\"\"\n    prompts = []\n    better_responses = []\n    worse_responses = []\n    \n    for _, row in train_data.iterrows():\n        prompt = row['prompt']\n        \n        # 好まれる回答と好まれない回答を特定\n        if row['winner_model_a'] == 1:  # モデルAが勝者\n            better_response = row['response_a']\n            worse_response = row['response_b']\n            prompts.append(prompt)\n            better_responses.append(better_response)\n            worse_responses.append(worse_response)\n            \n        elif row['winner_model_b'] == 1:  # モデルBが勝者\n            better_response = row['response_b']\n            worse_response = row['response_a']\n            prompts.append(prompt)\n            better_responses.append(better_response)\n            worse_responses.append(worse_response)\n            \n        # 同点の場合はスキップ（または別の処理を追加することも可能）\n        # 同点の場合も含める場合は以下のコメントを解除\n        #elif row['winner_tie'] == 1:\n            # 同点の場合の処理（例えば、両方とも「better」として扱うなど）\n    \n    return prompts, better_responses, worse_responses\n\n# 関数を使用してデータを抽出\nprompts, better_responses, worse_responses = extract_better_worse_responses(train_data)\n\nprint(f\"抽出されたサンプル数: {len(prompts)}\")\nprint(f\"最初の例:\\nプロンプト: {prompts[0][:100]}...\\n好まれる回答: {better_responses[0][:100]}...\\n好まれない回答: {worse_responses[0][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:10:08.513514Z","iopub.execute_input":"2025-03-08T14:10:08.513749Z","iopub.status.idle":"2025-03-08T14:10:11.202543Z","shell.execute_reply.started":"2025-03-08T14:10:08.513729Z","shell.execute_reply":"2025-03-08T14:10:11.201795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wandbをオフにするため、環境変数を設定\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n# モデルのロード\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n\n# 学習率の設定（通常はデフォルトより小さい値で開始）\nlearning_rate = 2e-5  # SBERTの微調整には2e-5〜5e-5が一般的\n\n# ウェイトディケイの設定\nweight_decay = 0.01  # 過学習防止のため\n\n# マージン値の設定（トリプレットロスのパラメータ）\nmargin = 0.5  # 正例と負例の埋め込み間の距離差をどれだけ取るか\n\n# トレーニングサンプルの作成\ntrain_examples = []\nfor prompt, better, worse in zip(prompts, better_responses, worse_responses):\n    train_examples.append(InputExample(texts=[prompt, better, worse]))\n\n# バッチサイズの設定（大きすぎるとメモリ不足に、小さすぎると学習が安定しない）\nbatch_size = 16\n\n# エポック数（多すぎると過学習するリスクあり）\nnum_epochs = 3\n\n# データローダーの作成\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n\n# トリプレットロスの設定（マージンを指定）\ntrain_loss = losses.TripletLoss(model=model, triplet_margin=margin)\n\n# 早期停止のコールバック設定\nfrom sentence_transformers.evaluation import TripletEvaluator\n# 検証用データセットを作成（全データの一部を使用）\ndev_samples = train_examples[:len(train_examples)//10]  # 10%を検証に使用\nevaluator = TripletEvaluator.from_input_examples(dev_samples)\n\n# オプティマイザの設定\noptimizer_params = {'lr': learning_rate, 'weight_decay': weight_decay}\n\n# ファインチューニングの実行\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=num_epochs,\n    evaluator=evaluator,\n    evaluation_steps=1000,  # 1000バッチごとに評価\n    warmup_steps=int(len(train_dataloader) * 0.1),  # 全体の10%をウォームアップに\n    optimizer_params=optimizer_params,\n    optimizer_class=torch.optim.AdamW,  # AdamWオプティマイザを使用\n    scheduler='WarmupLinear',  # 学習率スケジューラ\n    show_progress_bar=True,\n    output_path='fine-tuned-sbert-model',\n    save_best_model=True  # 最良のモデルを保存\n)\n# ファインチューニングされたモデルを使用してデータを再エンコード\nfine_tuned_embedding_p = model.encode(train_data[\"prompt\"])\nfine_tuned_embedding_A = model.encode(train_data[\"response_a\"])\nfine_tuned_embedding_B = model.encode(train_data[\"response_b\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:10:11.203312Z","iopub.execute_input":"2025-03-08T14:10:11.203545Z","iopub.status.idle":"2025-03-08T14:45:27.487136Z","shell.execute_reply.started":"2025-03-08T14:10:11.203522Z","shell.execute_reply":"2025-03-08T14:45:27.4864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('fine_tuned_embedding_p.npy', fine_tuned_embedding_p)\nnp.save('fine_tuned_embedding_A.npy', fine_tuned_embedding_A)\nnp.save('fine_tuned_embedding_B.npy', fine_tuned_embedding_B)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:45:27.489961Z","iopub.execute_input":"2025-03-08T14:45:27.490231Z","iopub.status.idle":"2025-03-08T14:45:27.665036Z","shell.execute_reply.started":"2025-03-08T14:45:27.490209Z","shell.execute_reply":"2025-03-08T14:45:27.664037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ParallelResponseEvaluator(nn.Module):\n    \"\"\"\n    プロンプトを基準に回答を並列評価するモデル\n    \n    プロンプトを基準として、回答AとBをそれぞれ独立に評価し、\n    その評価結果を比較して最終的な好まれやすさを予測します。\n    \"\"\"\n    def __init__(self, input_dim=384, hidden_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n        \"\"\"\n        初期化関数\n        \n        Args:\n            input_dim (int): 入力特徴量の次元数（SBERTの場合通常は384次元）\n            hidden_dim (int): 隠れ層の次元数\n            num_heads (int): 注意機構のヘッド数\n            num_layers (int): Transformerエンコーダー層の数\n            dropout (float): ドロップアウト率\n        \"\"\"\n        super(ParallelResponseEvaluator, self).__init__()\n        \n        # 特徴量の次元\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # プロンプト処理用のTransformerエンコーダー\n        self.prompt_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=input_dim,\n                nhead=num_heads,\n                dim_feedforward=hidden_dim,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=num_layers\n        )\n        \n        # クロスアテンション層（プロンプトと回答間の関係を処理）\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=input_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        # 回答固有の特徴を抽出するための変換層\n        self.response_feature_extractor = nn.Sequential(\n            nn.Linear(input_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # 比較層（回答AとBの特徴を比較）\n        self.comparison_layer = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # 出力層（3クラス分類）\n        self.classifier = nn.Linear(hidden_dim // 2, 3)\n        \n        # 補助的な評価モジュール（各回答の個別スコア計算用）\n        self.response_scorer = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n        \n        # モデルの重みを初期化\n        self._init_weights()\n        \n    def _init_weights(self):\n        \"\"\"モデルの重みを適切に初期化する関数\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    \n    def process_response(self, prompt_encoding, response_encoding):\n        \"\"\"\n        プロンプトと1つの回答の関係を処理する\n        \n        Args:\n            prompt_encoding (Tensor): プロンプトのエンコーディング [batch_size, 1, input_dim]\n            response_encoding (Tensor): 回答のエンコーディング [batch_size, 1, input_dim]\n            \n        Returns:\n            Tensor: 回答の特徴表現 [batch_size, hidden_dim]\n        \"\"\"\n        # クロスアテンション（プロンプトをクエリ、回答をキー・バリューとして）\n        # 注: クロスアテンションでは、プロンプトがクエリとして機能し、回答の関連部分に注目します\n        attn_output, _ = self.cross_attention(\n            prompt_encoding,  # クエリ: プロンプト\n            response_encoding,  # キー: 回答\n            response_encoding   # バリュー: 回答\n        )\n        \n        # 結合して豊かな特徴表現を作成\n        # プロンプトに基づく注意の結果と元の回答の両方を使用\n        combined = torch.cat([attn_output, response_encoding], dim=2)\n        combined = combined.squeeze(1)  # [batch_size, 1, input_dim*2] -> [batch_size, input_dim*2]\n        \n        # 特徴抽出\n        response_features = self.response_feature_extractor(combined)\n        \n        return response_features\n        \n    def forward(self, prompt_emb, response_a_emb, response_b_emb):\n        \"\"\"\n        順伝播関数\n        \n        Args:\n            prompt_emb (Tensor): プロンプトの埋め込み [batch_size, input_dim]\n            response_a_emb (Tensor): 回答Aの埋め込み [batch_size, input_dim]\n            response_b_emb (Tensor): 回答Bの埋め込み [batch_size, input_dim]\n            \n        Returns:\n            Tensor: 3クラスの確率 [batch_size, 3]\n        \"\"\"\n        batch_size = prompt_emb.size(0)\n        \n        # 次元を追加してTransformerに適した形に変換（シーケンス長1）\n        prompt_emb = prompt_emb.unsqueeze(1)  # [batch_size, 1, input_dim]\n        response_a_emb = response_a_emb.unsqueeze(1)  # [batch_size, 1, input_dim]\n        response_b_emb = response_b_emb.unsqueeze(1)  # [batch_size, 1, input_dim]\n        \n        # プロンプトのエンコーディング\n        prompt_encoding = self.prompt_encoder(prompt_emb)\n        \n        # 回答AとBを独立に処理\n        response_a_features = self.process_response(prompt_encoding, response_a_emb)\n        response_b_features = self.process_response(prompt_encoding, response_b_emb)\n        \n        # 個別のスコア計算（補助的な予測として）\n        score_a = self.response_scorer(response_a_features)\n        score_b = self.response_scorer(response_b_features)\n        \n        # 回答AとBの特徴を結合して比較\n        comparison_features = torch.cat([response_a_features, response_b_features], dim=1)\n        comparison_output = self.comparison_layer(comparison_features)\n        \n        # 最終的な分類（3クラス）\n        logits = self.classifier(comparison_output)\n        \n        # スコア差を考慮した調整（オプション）\n        score_diff = score_a - score_b\n        # ここではスコア差を直接利用しないが、モデルの拡張として使用可能\n        \n        return logits, score_a, score_b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:45:27.666043Z","iopub.execute_input":"2025-03-08T14:45:27.666338Z","iopub.status.idle":"2025-03-08T14:45:27.677421Z","shell.execute_reply.started":"2025-03-08T14:45:27.666308Z","shell.execute_reply":"2025-03-08T14:45:27.676588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualEncoderModel(nn.Module):\n    \"\"\"\n    デュアルエンコーダーアーキテクチャを使用した好まれやすさ予測モデル\n    \n    プロンプトと回答のペアをそれぞれ評価し、その結果を比較します。\n    回答AとBは順序によらず同等に扱われます。\n    \"\"\"\n    def __init__(self, input_dim=384, hidden_dim=256, dropout=0.1):\n        \"\"\"\n        初期化関数\n        \n        Args:\n            input_dim (int): 入力特徴量の次元数（SBERTの場合通常は384次元）\n            hidden_dim (int): 隠れ層の次元数\n            dropout (float): ドロップアウト率\n        \"\"\"\n        super(DualEncoderModel, self).__init__()\n        \n        # プロンプト-回答のマッチングを評価するエンコーダー\n        self.pair_encoder = nn.Sequential(\n            nn.Linear(input_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        # ペアの特徴から個別スコアを計算\n        self.pair_scorer = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n        \n        # 好まれやすさの差から最終的な分類を行う層\n        self.preference_classifier = nn.Sequential(\n            nn.Linear(hidden_dim * 2 + 2, hidden_dim),  # 2つのペア特徴+スコア差\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 3)  # 3クラス分類\n        )\n        \n        # モデルの重みを初期化\n        self._init_weights()\n        \n    def _init_weights(self):\n        \"\"\"モデルの重みを適切に初期化する関数\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n        \n    def encode_pair(self, prompt_emb, response_emb):\n        \"\"\"\n        プロンプトと回答のペアをエンコードする\n        \n        Args:\n            prompt_emb (Tensor): プロンプトの埋め込み [batch_size, input_dim]\n            response_emb (Tensor): 回答の埋め込み [batch_size, input_dim]\n            \n        Returns:\n            tuple: (ペアの特徴, 適合度スコア)\n        \"\"\"\n        # プロンプトと回答を結合\n        pair_input = torch.cat([prompt_emb, response_emb], dim=1)\n        \n        # ペアの特徴を抽出\n        pair_features = self.pair_encoder(pair_input)\n        \n        # 適合度スコアを計算\n        score = self.pair_scorer(pair_features)\n        \n        return pair_features, score\n        \n    def forward(self, prompt_emb, response_a_emb, response_b_emb):\n        \"\"\"\n        順伝播関数\n        \n        Args:\n            prompt_emb (Tensor): プロンプトの埋め込み [batch_size, input_dim]\n            response_a_emb (Tensor): 回答Aの埋め込み [batch_size, input_dim]\n            response_b_emb (Tensor): 回答Bの埋め込み [batch_size, input_dim]\n            \n        Returns:\n            Tensor: 3クラスの確率 [batch_size, 3]\n        \"\"\"\n        # プロンプト-回答Aのペアを評価\n        pair_a_features, score_a = self.encode_pair(prompt_emb, response_a_emb)\n        \n        # プロンプト-回答Bのペアを評価\n        pair_b_features, score_b = self.encode_pair(prompt_emb, response_b_emb)\n        \n        # スコアの差を計算\n        score_diff = score_a - score_b\n        score_abs_diff = torch.abs(score_diff)\n        \n        # すべての特徴を結合\n        combined_features = torch.cat([\n            pair_a_features, \n            pair_b_features, \n            score_diff,  # 方向性のある差（A-B）\n            score_abs_diff  # 絶対的な差（同等かどうかの判断に役立つ）\n        ], dim=1)\n        \n        # 最終的な分類\n        logits = self.preference_classifier(combined_features)\n        \n        return logits, score_a, score_b\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:45:27.679296Z","iopub.execute_input":"2025-03-08T14:45:27.679593Z","iopub.status.idle":"2025-03-08T14:45:27.697207Z","shell.execute_reply.started":"2025-03-08T14:45:27.67957Z","shell.execute_reply":"2025-03-08T14:45:27.696434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# データセットクラス（より明示的な並列構造用）\nclass ParallelResponseDataset(Dataset):\n    \"\"\"\n    回答を並列に扱うためのデータセットクラス\n    \"\"\"\n    def __init__(self, prompt_embeddings, response_a_embeddings, response_b_embeddings, labels=None):\n        \"\"\"\n        初期化関数\n        \n        Args:\n            prompt_embeddings (numpy.ndarray): プロンプトの埋め込み [N, embed_dim]\n            response_a_embeddings (numpy.ndarray): 回答Aの埋め込み [N, embed_dim]\n            response_b_embeddings (numpy.ndarray): 回答Bの埋め込み [N, embed_dim]\n            labels (numpy.ndarray, optional): ラベル [N]\n                                             0: Aが好まれる, 1: Bが好まれる, 2: 同等\n        \"\"\"\n        self.prompt_embeddings = torch.tensor(prompt_embeddings, dtype=torch.float32)\n        self.response_a_embeddings = torch.tensor(response_a_embeddings, dtype=torch.float32)\n        self.response_b_embeddings = torch.tensor(response_b_embeddings, dtype=torch.float32)\n        \n        if labels is not None:\n            self.labels = torch.tensor(labels, dtype=torch.long)\n        else:\n            self.labels = None\n            \n        self.has_labels = labels is not None\n        \n    def __len__(self):\n        \"\"\"データセットの長さを返す\"\"\"\n        return len(self.prompt_embeddings)\n    \n    def __getitem__(self, idx):\n        \"\"\"インデックスに対応するデータを返す\"\"\"\n        if self.has_labels:\n            return (\n                self.prompt_embeddings[idx],\n                self.response_a_embeddings[idx],\n                self.response_b_embeddings[idx],\n                self.labels[idx]\n            )\n        else:\n            return (\n                self.prompt_embeddings[idx],\n                self.response_a_embeddings[idx],\n                self.response_b_embeddings[idx]\n            )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:45:27.69812Z","iopub.execute_input":"2025-03-08T14:45:27.698336Z","iopub.status.idle":"2025-03-08T14:45:27.709115Z","shell.execute_reply.started":"2025-03-08T14:45:27.69831Z","shell.execute_reply":"2025-03-08T14:45:27.708346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 訓練関数\ndef train_parallel_model(model, train_loader, val_loader, n_epochs=20, lr=0.001, \n                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \"\"\"\n    並列処理モデルを訓練する関数\n    \n    Args:\n        model: 訓練するモデル\n        train_loader: 訓練データローダー\n        val_loader: 検証データローダー\n        n_epochs: エポック数\n        lr: 学習率\n        device: 使用デバイス ('cuda' or 'cpu')\n        \n    Returns:\n        dict: 訓練結果（ベストモデル、履歴など）\n    \"\"\"\n    # モデルをデバイスに移動\n    model = model.to(device)\n    print(f\"Using device: {device}\")\n    \n    # 損失関数と最適化アルゴリズム\n    criterion = nn.CrossEntropyLoss()\n    aux_criterion = nn.MSELoss()  # 補助的なスコア予測用\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # 学習率スケジューラ\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n    )\n    \n    # 結果記録用の変数\n    best_val_acc = 0.0\n    best_model_state = None\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': [],\n        'learning_rates': []\n    }\n    \n    # エポックごとの訓練ループ\n    for epoch in range(n_epochs):\n        # 現在の学習率を記録\n        current_lr = optimizer.param_groups[0]['lr']\n        history['learning_rates'].append(current_lr)\n        \n        print(f\"\\nEpoch {epoch+1}/{n_epochs}, LR: {current_lr:.6f}\")\n        \n        # 訓練フェーズ\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for batch_data in train_loader:\n            prompt_emb, resp_a_emb, resp_b_emb, labels = [b.to(device) for b in batch_data]\n            \n            # 勾配をゼロに初期化\n            optimizer.zero_grad()\n            \n            # 順伝播\n            if isinstance(model, ParallelResponseEvaluator) or isinstance(model, DualEncoderModel):\n                outputs, score_a, score_b = model(prompt_emb, resp_a_emb, resp_b_emb)\n                \n                # メイン損失（分類）\n                main_loss = criterion(outputs, labels)\n                \n                # 補助的な損失（スコア予測）\n                # ラベルに基づいて期待されるスコアの差を設定\n                # 0: Aが好まれる -> スコア差を正に\n                # 1: Bが好まれる -> スコア差を負に\n                # 2: 同等 -> スコア差を0に\n                expected_scores = torch.zeros_like(score_a)\n                expected_scores[labels == 0] = 1.0  # Aが好まれる\n                expected_scores[labels == 1] = -1.0  # Bが好まれる\n                # 同等の場合は0のまま\n                \n                aux_loss = aux_criterion(score_a - score_b, expected_scores)\n                \n                # 総合損失\n                loss = main_loss + 0.5 * aux_loss  # 補助損失の重みは調整可能\n            else:\n                outputs = model(prompt_emb, resp_a_emb, resp_b_emb)\n                loss = criterion(outputs, labels)\n            \n            # 逆伝播と最適化\n            loss.backward()\n            optimizer.step()\n            \n            # 統計の更新\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n        \n        # 訓練統計の計算\n        train_loss /= len(train_loader)\n        train_acc = train_correct / train_total\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        \n        # 検証フェーズ\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for batch_data in val_loader:\n                prompt_emb, resp_a_emb, resp_b_emb, labels = [b.to(device) for b in batch_data]\n                \n                # 順伝播\n                if isinstance(model, ParallelResponseEvaluator) or isinstance(model, DualEncoderModel):\n                    outputs, _, _ = model(prompt_emb, resp_a_emb, resp_b_emb)\n                else:\n                    outputs = model(prompt_emb, resp_a_emb, resp_b_emb)\n                \n                # 損失の計算\n                loss = criterion(outputs, labels)\n                \n                # 統計の更新\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                \n                # 予測とターゲットを記録\n                val_preds.extend(predicted.cpu().numpy())\n                val_targets.extend(labels.cpu().numpy())\n        \n        # 検証統計の計算\n        val_loss /= len(val_loader)\n        val_preds = np.array(val_preds)\n        val_targets = np.array(val_targets)\n        val_acc = accuracy_score(val_targets, val_preds)\n        \n        # 混同行列を計算\n        conf_matrix = confusion_matrix(val_targets, val_preds)\n        \n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(conf_matrix)\n        \n        # 学習率スケジューラを更新\n        scheduler.step(val_acc)\n        \n        # ベストモデルの保存\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = model.state_dict().copy()\n            print(f\"New best model with validation accuracy: {val_acc:.4f}\")\n    \n    # 訓練終了後、ベストモデルを復元\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n        \n    return {\n        'model': model,\n        'best_val_acc': best_val_acc,\n        'history': history\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:45:27.710106Z","iopub.execute_input":"2025-03-08T14:45:27.710393Z","iopub.status.idle":"2025-03-08T14:45:27.732272Z","shell.execute_reply.started":"2025-03-08T14:45:27.710364Z","shell.execute_reply":"2025-03-08T14:45:27.731375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_embeddings = fine_tuned_embedding_p\nresponse_a_embeddings = fine_tuned_embedding_A\nresponse_b_embeddings = fine_tuned_embedding_B\n\nnum_samples = prompt_embeddings.shape[0]\nembed_dim = prompt_embeddings.shape[1]\n\nprint(num_samples)\nprint(embed_dim)\n\n\n# ラベルがGPU上にある場合\nif isinstance(labels, torch.Tensor) and labels.is_cuda:\n    labels_cpu = labels.cpu().numpy()\nelse:\n    labels_cpu = labels\n\ntrain_idx, test_idx = train_test_split(\n    np.arange(num_samples), test_size=0.2, random_state=42, stratify=labels_cpu\n)\n\ntrain_idx, val_idx = train_test_split(\n        train_idx, test_size=0.25, random_state=42, stratify=labels[train_idx]\n)\n    \n\n# インデックスに基づいてデータを分割\np_train, r_a_train, r_b_train = prompt_embeddings[train_idx], response_a_embeddings[train_idx], response_b_embeddings[train_idx]\np_val, r_a_val, r_b_val = prompt_embeddings[val_idx], response_a_embeddings[val_idx], response_b_embeddings[val_idx]\np_test, r_a_test, r_b_test = prompt_embeddings[test_idx], response_a_embeddings[test_idx], response_b_embeddings[test_idx]\n\ny_train, y_val, y_test = labels[train_idx], labels[val_idx], labels[test_idx]\n\n# ========== データローダーの作成 ==========\n# データセットの作成\ntrain_dataset = ParallelResponseDataset(p_train, r_a_train, r_b_train, y_train)\nval_dataset = ParallelResponseDataset(p_val, r_a_val, r_b_val, y_val)\ntest_dataset = ParallelResponseDataset(p_test, r_a_test, r_b_test, y_test)\n\n# データローダーの作成\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n\n# ========== モデルの作成と訓練 ==========\n# どちらのモデルを使用するか選択\nmodel_type = \"parallel_evaluator\"  # または \"parallel_evaluator\"\n\nif model_type == \"parallel_evaluator\":\n    model = ParallelResponseEvaluator(\n        input_dim=embed_dim,      # 入力の次元数\n        hidden_dim=256,           # 隠れ層の次元数\n        num_heads=4,              # 注意ヘッドの数\n        num_layers=2,             # エンコーダー層の数\n        dropout=0.1               # ドロップアウト率\n    )\nelse:  # dual_encoder\n    model = DualEncoderModel(\n        input_dim=embed_dim,      # 入力の次元数\n        hidden_dim=256,           # 隠れ層の次元数\n        dropout=0.1               # ドロップアウト率\n    )\n\n# モデルの要約を表示\nprint(model)\n\n# デバイスの設定\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# モデルの訓練\ntraining_results = train_parallel_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    n_epochs=20,          # エポック数\n    lr=0.001,             # 初期学習率\n    device=device         # 使用デバイス\n)\n\n# ========== モデルの評価 ==========\n# テストデータでの評価\nmodel.eval()\ntest_preds = []\ntest_targets = []\n\nwith torch.no_grad():\n    for batch_data in test_loader:\n        prompt_emb, resp_a_emb, resp_b_emb, labels = [b.to(device) for b in batch_data]\n        \n        # 順伝播\n        if isinstance(model, ParallelResponseEvaluator) or isinstance(model, DualEncoderModel):\n            outputs, _, _ = model(prompt_emb, resp_a_emb, resp_b_emb)\n        else:\n            outputs = model(prompt_emb, resp_a_emb, resp_b_emb)\n            \n        _, predicted = torch.max(outputs, 1)\n        test_preds.extend(predicted.cpu().numpy())\n        test_targets.extend(labels.cpu().numpy())\n\n# 精度の計算\ntest_acc = accuracy_score(test_targets, test_preds)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# 混同行列の表示\nconf_matrix = confusion_matrix(test_targets, test_preds)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T14:49:02.707435Z","iopub.execute_input":"2025-03-08T14:49:02.707745Z","iopub.status.idle":"2025-03-08T14:53:53.4844Z","shell.execute_reply.started":"2025-03-08T14:49:02.707723Z","shell.execute_reply":"2025-03-08T14:53:53.483618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}